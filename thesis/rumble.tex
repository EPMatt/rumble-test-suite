\chapter{Background and Related work}
In this chapter, we will introduce context on which our work is based. For full overview, we must familiarize the reader with the following concepts: Big Data, NoSQL, MapReduce, YARN, Spark, JSON, JSONiq and finally Rumble.

Test Driver itself will be built as a layer on top of Rumble. Because of the architecture which enables data independence, we do not need to know its under-laying structure. However, seeing the full architecture and having an overview will help us make decisions throughout this work.

\section{Big Data}
Big Data in today's world has a broad scope and several definitions. Here we will present a certain view of the Big Data on which Rumble was based. We can look at the data being "big" in following 3 dimensions \cite{BigDataCourse}\todo{from 01 Big Data - Introduction although I updated from 2020 lecture}: 
\begin{itemize}
	\item Volume - These term simply corresponds to the amount of bytes that our data consists of. To have idea of the scale, in Big Data we are often looking at PB of data. Scientific centers such as CERN produce tens of PB of data annually. The information, the data in today's world brings value. Not only scientific centers, but also big companies gather data, store it in their data centers and process it in order to extract this value.
	\item Variety - Data often comes in different shapes. The most familiar ones are Text - completely unstructured data, followed by data organized as Cubes, Tables, Graphs or Trees on which we will mainly focus. Until 2000's, the world was mainly oriented towards Relational Databases for which they under-laying shape is Table. Main focus was on introducing normalization forms with the idea to avoid data redundancy. Then the tables would simply be joined using SQL as the query language via the foreign keys. However, starting from 2000's Relational Databases and SQL could not satisfy the needs of real world scenarios. Often data is unstructured, nested, values are missing etc. This trend led to NoSQL Databases. Main focus in NoSQL Database is to perform opposite and actually de-normalize the data. Looking at the table, we would now allow non-atomic values in a single cell or even missing values. Such a transition leads the data shape to transform from flat homogeneous tables to nested heterogeneous trees. Choosing the correct data shape is essential. What CSV and SQL were in relational database, for tree shaped data we have JSON and XML as a data format with JSONiq and XQuery as their respective querying languages.
	\item Velocity - Data in the end is physically stored on some medium drive. The 3 main factors of this under-laying medium drive are Capacity, Throughput and Latency. From mid 1950's until today we have witnessed tremendous increase in all 3 factors. Capacity has increased by up to 200 x $10^9$, throughput by 10 x $10^3$  and latency by 8 times. This ever-growing discrepancy between factors has brought needs for parallelization and batch processing. Since a single medium drive has increased capacity much more compared to throughput we need to read data from multiple medium drives at the same time in parallel to be able to obtain data fast enough. At the same time, to face discrepancy between throughput and latency, we need to obtain data in batches. Thus, the need for systems that can perform parallel batch processing has increased. \todo{maybe insert a picture from presentation???}
\end{itemize}
In summary, traditional RDBMS such as Oracle Database or Microsoft SQL Server, have focused on being complaint with ACID (Atomicity, Consistency, Isolation and Durability) properties. Such RDBMS with homogeneous tables are good when handling small amount of data. However, if we need to massively scale the data, we need to turn to different technologies. These traditional RDBMS that use File Systems such as FAT32 or NTFS for physical storage are not sufficient. \todo{from 02 Big Data - Lessons learn}

NoSQL (Not Only SQL) Databases on the other hand are compliant with CAP (Capability, Availability, Partition tolerance) theorem.  Examples of new NoSQL databases that have emerged are key-value stores (DynamoDB), column-oriented stores (HBase) and document stores (MongoDb). They often use Distributed File System as physical storage such as HDFS. Instead of traditional scaling up, by buying single high-performance hardware, the orientation is towards scaling out by buying a lot of cheap commodity hardware. Such scaling enables that hardware costs grow linearly with the amount of data. All these concepts lead to building high performance and scale-able frameworks such as Hadoop that can query and process distributed massive data in parallel. \todo{from 03 Data in the Large - Object and Key-Value Storage}

\section{Hadoop}
Apache Hadoop \cite{Hadoop} is an open source framework written in Java that is able to manage big data, store it and process it in parallel in a distributed way across cluster of commodity hardware. It consists of 3 components:
\begin{itemize}
	\item HDFS - Storage layer\todo{from 04 Data in the Large - Distributed File systems and Exercise03 HDFS}
	\item MapReduce - Processing layer
	\item YARN - Resource management layer
\end{itemize}

In this section we will briefly introduce each of the layers. It will help reader to better understand Spark in the upcoming chapter.

\subsection{HDFS}
Hadoop Distributed File System - HDFS \cite{HDFS} is a physical storage layer of Hadoop inspired by GFS \cite{GFS} written in Java. It is one of the most reliable FS for storing big data distributed on a cluster of commodity hardware. 

In this section, we need to understand how HDFS physically stores big data on the machines. When we say big data, we are thinking at scale of millions of PB files. This means that files are bigger than single drive (medium). Therefore, in such a setting, the most suitable is block storage. Unlike typical NTFS system with allocation units of 4 KB, the block size is by default 64 or 128 MB. It is chosen as a good trade off between latency and replication. Transferring multiple blocks bigger than 4 KB will reduce latency and also reduce the network overhead. When it comes to replicas, each block has by default 3 copies in case of a failure.  

The architecture is master-slave. Master is NameNode and it is in charge of storing the namespace. Nemaspace is hierarchy of files and directories. Since the blocks are 64 or 128 MB, the metadata is also small. And since we are storing rather small amount of very large files, the whole namespace can fit in RAM of the NameNode. In addition to namespace, NameNode knows the file to block it consists of mapping together with location of block and its replicas. The blocks are stored on DataNodes that act as slaves. When clients want to read/write the files they communicate with NameNode only once to receive the locations meaning that NameNode is not the bottleneck. 

Such an architecture allows potential infinite scalability just by adding DataNodes meaning that hardware cost grows linearly with the increase of data. The single point of failure is NameNode, meaning that from CAP theorem we have Capability and Partition tolerance at the cost of Availability. In case of a failure, there is a secondary NameNode that would start-up. Also it enables high durability with 3 replicas and read/write performance. When reading, it usually transfers a lot of data - batch processing.

\subsection{MapReduce}
MapReduce \cite{MapReduce} in most broad definition is a programming model (style). It answers to the question how we process the data and it consists of two crucial steps map and reduce alongside with shuffle as the intermediate step: \todo{from 06 Data in the Large - Massive Parallel Processing}
\begin{itemize}
	\item Map - Input data is mapped into intermediate set of key-value pairs
	\item Shuffle - All key-value pairs are shuffled in a way such that all pairs with the same key end up same machine
	\item Reduce - Data is aggregated on the machine and the output is produced
\end{itemize}
\todo{Maybe insert picture left to right Map Sort Partition Reduce}
Example - Count occurrences of each word in a document of 1000 Pages:

\begin{enumerate}
	\item What we can do is that we can first have a single map task per a page. This way those 1000 pages can be done in parallel. The map task will perform Map (K1, V1) $->$ List (K2, V2), where K1 is in range from 1 to 1000 (for each page) and V1 is the text on each page. K2 will have values in range of all possible words that occur in the document. V2 will always be 1. Such a mapper is very primitive. In case that reduce task is a function that is commutative and associative then it is allowed to execute same function in map task to reduce the amount of shuffle that will happen afterwards. As count is such a function, in map task we can already perform sum per key. It means that K2 stays same and V2 will be the actual count per page!
	\item As not all possible words will appear in all pages, we will simply put together collection of all the produced key value pairs and sort them per key. We will then assign all key-value pairs with the same key to the single reducer and partition the data accordingly.
	\item Reduce task will perform (K2, List (V2)) $->$ List (K2, V3) - Reducer can output the same key value pair, but in general it can be any other. Finally V3 will be the sum of occurrences of the word K2!
\end{enumerate}
   
In general MapReduce as programming model can be used in any framework with any under-laying physical storage such as Local File System, S3, Azure, HDFS. Here we will describe infrastructure in Hadoop version 1 where it is running on top of HDFS where we also have Resource Management layer. It is again master-slave architecture where we have JobTracker and TaskTracker. JobTracker is the master with responsibilities of Resource Management, Scheduling, Monitoring, Job lifecycle and Fault-tolerance. One Job contains from multiple tasks, depending on how the data is split. And 1 task can be map or reduce task. 1 or more tasks are then assigned to TaskTracker that need to execute them. JobTracker is collocated with NameNode and TaskTracker usually with the DataNode in order to bring query to the data. 

\subsection{YARN}
Yet Another Resource Negotiator -  YARN \cite{YARN} is a Resource Management layer in Hadoop Version 2. Comparing to Version 1, we might notice that JobTracker has a lot of responsibilities. It is responsible for both types of jobs, scheduling and monitoring ones. In such a setting, JobTracker is acting as the "Jack of all trades" and becoming a bottleneck. This introduces scale-ability issues such Hadoop could not handle more than 4000 nodes executing more than 40000 tasks (remember that job comprises a set of task). 

This is the reason of introducing YARN. YARN clearly separates scheduling and monitoring responsibilities. The architecture is again master-slave where we have ResourceManager and NodeManager. There is a single ResourceManager per cluster that is in charge of only scheduling and performs: Capacity guarantees, Fairness, SLA, Cluster Utilization, Assigns containers. It has global overview of all cluster resources and provides leases for containers. One node in a cluster has one NodeManager and many Containers. Container is abstraction in which task can be run and it comprises a set of resources such as RAM, CPU, Storage, Bandwidth that can be allocated to ApplicationMaster. ApplicationMaster has the responsibility to handle monitoring. In particular it is in charge of: Fault tolerance, Monitoring, Asking for resources, Tracing job progress/status, Heartbeating to resource manager, Ability to handle multiple jobs. We have many ApplicationMasters in a cluster, each job has 1 application master, but not every node has to have a ApplicationMaster. In essence it can happen that single node has multiple ApplicationMasters, each responsible for different job completely unaware of the existence of other ApplicationMasters on the node. Finally it is should be noted that ApplicationMaster is a container. Described architecture solves the bottleneck issue allowing cluster to scale up to 10000 nodes and 100000 tasks. \todo{from 07 Data in the Large - Resource Management and Exercise 06 Spark}

Full flow of duties overview:
\begin{itemize}
	\item Clients submits a job. 
	\item ResourceManager creates a job and returns ID. \item Client sends its requirements. 
	\item ResourceManager tells a NodeManager to promote one of containers to ApplicationMaster. 
	\item ResourceManager tells maximum capacity of containers. 
	\item ApplicationMaster requests containers. 
	\item ResourceManager assigns containers \todo{Maybe architecture picture with containers and all}
\end{itemize}


YARN offers couple of types of schedulers that based on application and its request in terms of resources perform allocation.

\section{Spark}
Apache Spark \cite{ApacheSpark} \cite{SparkDefinitiveGuide} \cite{LearningSpark} is is an open-source engine for large-scale data processing. We see it as generalization of Map Reduce. From straight pipeline of two tasks, map and reduce, it generalizes it to any Directed Acyclic Graph - DAG. DAGs are built around Resilient Distributed Datasets - RDDs \cite{RDD} which are abstraction for partitioned collection of values. On RDDs, we can perform creation, transformation and action. In Spark we need to make a clear separation of two plans, two graphs - lineage and DAG.

DAG is basically a physical plan of execution. A DAG is created when the user creates a RDD (by referencing a dataset in an external File System for example) and applies chains of lazy transformations on it. When action is called, it triggers the computation. The DAG is given to the DAG Scheduler which divides it into stages of tasks. A stage is comprised of tasks based on partitions of the input data. The Stages are passed on to YARN that now executes it physically. Since Spark has end to end DAG, it can figure out which tasks can be done in parallel. All these will then run into parallel on several nodes. \todo{from 08 Data in the Large - Massive Parallel Processing (SPARK) and Exercise 06 Spark}

Lineage graph tells us a logical plan. It tells us which RDD originates from which RDD. All the dependencies between the RDDs will be logged in lineage graph, rather than the actual data. This is called lazy evaluation, it only gets triggered when action is called. This lineage is used to recompute the RDD in case of failure.

Fault tolerance using lineage - Imagine that we start with a RDD on which we need to perform couple of transformations and finally an action. Such RDD would first get partitioned so that it can be handled by multiple nodes. Imagine that some node fails, it means that only the partitions that were on that node have to be recomputed. And lineage graph is telling us exactly which set of transformation is needed to reconstruct the RDD. \todo{Maybe a picture of Lineage from P8 - Spark adding on side what is RDD1, RDD2 … RDD4 and how could they be distributed}

DataFrame is high level abstraction of RDD's. It is logical data model that enables users to view and manipulate data independently of physical storage. DataFrames store data in collection of rows enabling user to look at RDD's as tables. They are nothing more than named columns like we had before. Therefore, we can use high level declarative language - Spark SQL to query the data without caring about under-laying physical storage.

The main problem with DataFrames is that heterogeneous data that we are encountering in tree data shapes cannot fit in DataFrame. All the de-normalization that enabled nested, missing values or values of different type will not work. Running Spark on such a DataSet results in Spark skipping and leaving to user to manually handle heterogeneous data. DataFrames are simply not the correct representation for the tree shaped data. 

\subsection{Apache Spark vs Apache Hadoop MapReduce}
For emphasizing power of Spark, we have found a nice comparison with Hadoop MapReduce that can be separated in following categories: \todo{how to insert YouTube link here https://www.youtube.com/watch?v=ivgQtdB-BS4}
\begin{itemize}
	\item Performance - Hadoop MapReduce stores the output on the disk after each map or reduce task. Spark keeps everything in memory. Spark performs better if all data is stored in RAM. If RAM is full, Spark uses disk but overall it is better.
	\item Ease of use - Spark has compatible API for Python, Scala, Java. On the other hand, Hadoop MapReduce is written in Java and it is hard to learn the syntax for programming. 
	\item Cost - Spark needs a lot of RAM so it is more expensive. All data needed for job has to fit in RAM
	\item Data processing - Spark can do graph, ML, batch and real time processing which makes it one platform for everything. Hadoop MapReduce is good for batch processing, but it doesn't support graph or real time processing. 
	\item Fault tolerance - Hadoop MapReduce relies on hard drives. In case of failure, it can continue wherever it left of and save time. It also has replication for fault tolerance. Spark uses RDDs for fault tolerance. They can refer to any dataset in external storage like HDFS. If RDD is lost it is recomputed using transformations.
\end{itemize}

\section{Querying Language}
\subsection{JSON}
\label{sec:JSON}
JavaScript Object Notation - JSON \cite{JSON}is a text only, human-readable data format. It originates from JavaScript, but today it is a widely spread language-independent data format supported by many programming languages. \todo{Maybe a picture of a JSON document}

As we have seen DataFrames in Spark and table data shape in general that can be stored in CSV data format, is not suitable for heterogeneous data and de-normalization does not work. On the other hand, tree data shape and JSON as data format in particular, is perfect choice for nested heterogeneous data. It supports nesting by using 2 structured data types:
\begin{itemize}
	\item Object - collection of key-value pairs that acts as associative array (map) from string to any other type 
	\item Arrays - ordered sequence of items of any type. 
\end{itemize} 

JSON also supports the 4 Atomic data types that can be String, Number, Boolean and Null.

\subsection{JSONiq}
\label{sec:JSONiq}
JSONiq \cite{JSONiqPaper} as mentioned in the introduction is declarative and functional querying language created exactly to analyze files written in JSON data format. It is designed to analyze tree shaped data - nested and heterogeneous. It inherits 95\% of its features from XQuery, its XML counterpart. It has data model that is able to capture all aspects of JSON data format. 

We say it is declarative because user does not be aware of the under-laying structure. It is a query language like SQL is in the RDBMS, with a difference that it operates on JSON.

When it comes to data model, everything is expressed as a Sequence of Items. Item itself can be any of the 6 data types that JSON supports. In addition, Item can also be of a Function Type. Then all Expressions that exist operate only on Sequence of Items. 

We say it is functional because Expression takes Sequence of Items as the input and as the output produces again Sequence of Items. This means that Expressions can be nested in any desired way.

The Expression can be :
\begin{itemize}
	\item Arithmetic
	\item Logic
	\item Comparison
	\item Literal
	\item JSON construction
	\item JSON navigation
	\item Sequence Construction
	\item Built-in function
	\item FLWOR expression.
\end{itemize} 

FLWOR expression is the most powerful. Using its own clauses, it is capable everything Select From Where in SQL does - Selection, Projection, Grouping, Ordering, Join. In addition, that it can be nested any number of times in almost any order which SQL does not quite support. \cite{JSONiqBook}
\todo{Maybe image with example queries}

Tuple stream is produced by each clause in the FLWOR expression. It is a set of key-value pairs representing a binding from variable name to corresponding Sequence of Items. The clauses can consume these tuple streams and produce tuple streams. So between themselves, clauses communicate via tuple streams. As we said that all Expressions operate on Sequence of Items, only return clause that always has to be included in every FLWOR expression will actually consume tuple steam and produce Sequence of Items. \cite{JSONiqBook}
   
\section{Rumble}
This chapter provides a high lever overview of architecture and how Rumble works. Then it explains how exactly JSONiq and Spark are glued together via Rumble and which mappings were performed to make it happen. Finally the General Architecture of Rumble engine is explained.
\subsection{User Perspective}
The user can use Rumble via command line or using the Rumble API for Java. The architecture overview is quite simple and presented in Figure \ref{fig:Rumble_Architecture}. User only sees JSONiq query language and uses it to write desired query. Rumble then takes this query and it has logic capable to map and pass the query down to Spark. Spark is then able to execute query in the cluster. Spark usually reads from DFS, most typically HDFS we mentioned before. But more in general it can run on any FS or database. Typical input for a query is JSON Lines document. JSON Lines document uses JSON data format and the only difference from typical JSON document is that every line in the document is a single object. Such document has a bit lower human-readability for nested data compared to JSON document but it is quite commonly used in other fields such as Web Programming. \cite{RumbleYouTube}

\begin{figure}[h!]
	\vspace*{-5mm}
	\includegraphics[width=\linewidth]{rumble_architecture.png}
	\vspace*{-10mm}
	\caption{Rumble Architecture Overview}
	\label{fig:Rumble_Architecture}
\end{figure}

\subsection{Mapping}
We said that Rumble has a logic that is capable to map query to Spark primitives. We also said that in JSONiq, everything is Sequence of Items. Therefore, Rumble uses interface Item in the code \cite{RumbleRepository}. All 6 types that were mentioned in Section \ref{sec:JSON} then implement this interface. After that, Item is wrapped using the Spark JavaRDD generic class and the mapping is complete! Spark is now able to execute queries using objects of the wrapper class.\todo{Maybe I should present the full inheritance tree with subtypes as well}

We also said that out of all Expressions, FLWOR Expressions are the most powerful ones and we can view them as set of clauses. Between themselves, clauses operate by consuming tuple streams instead of operating on Sequence of Items. Sequence of Items is produced only in the end with mandatory Return clause. Therefore, in the code \cite{RumbleRepository},Rumble uses class FlworTuple for wrapping to the Spark JavaRDD generic class similarly like already explained for types. For each clause, we have a Spark Iterator and they all, with exception of Return, have a reference to FlworTuple. 

As explained by Irimescu in \cite{RumbleThesis}: "Clauses can be mapped to a set of RDD transformations (potentially containing RDD creations as well) executed on RDDs that contain maps of variable names and sequences encapsulated in tuple objects. A summary of the mappings from clauses to transformations is presented below in Table \ref{tab:ClausesMapping}:"
\todo{This part is a bit suspicious as it seems that code evolved a bit. It is not clear to me whether we wrap FlworTuple and push it down to Spark or is it executed locally and what locally exactly means}
\begin{table}[h!]
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|l|l|}
			\hline
			\multicolumn{2}{|l|}{\textbf{Theoretical Mapping - Clauses to Transformations/Actions}}                                                                                                                                                  \\ \hline
			\begin{tabular}[c]{@{}l@{}}FLWOR \\ Clause\end{tabular} & \begin{tabular}[c]{@{}l@{}}Spark Transformation \\ / Action\end{tabular}                                                                                                       \\ \hline
			\multirow{2}{*}{for}                                    & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}flatMap() - map each incoming tuple to a set of tuples,\\ each extending the original one with one new key-value pair\end{tabular}} \\
			&                                                                                                                                                                                \\ \hline
			let                                                     & map() - extend each incoming tuple with the new variable name to sequence of items pair                                                                                        \\ \hline
			where                                                   & filter(condition)                                                                                                                                                              \\ \hline
			\multirow{3}{*}{order by key}                           & 1. mapToPair() - map each tuple to a pair with the sort key and the tuple itself                                                                                               \\ \cline{2-2} 
			& 2. sortByKey()                                                                                                                                                                 \\ \cline{2-2} 
			& 3. map() - map back from pairs to tuples only                                                                                                                                  \\ \hline
			\multirow{3}{*}{group by key}                           & 1. mapToPair() - map each tuple to a pair of group key and the tuple itself                                                                                                    \\ \cline{2-2} 
			& 2. groupByKey()                                                                                                                                                                \\ \cline{2-2} 
			& 3. map() - map back from pairs to tuples only and linearize the results                                                                                                        \\ \hline
			return                                                  & map() + collect()/take()                                                                                                                                                       \\ \hline
		\end{tabular}%
	}
	\caption{Phase 1 Results Overview}
	\label{tab:ClausesMapping}
\end{table}

\subsection{General Architecture}
So far, we were referring to Rumble as an engine. Essentially it is a compiler implemented in Java and as such it follows basic Compiler Design principles. In order not to break declarative property of JSONiq query language, it requires a proper separation of concerns. Irimescu in his thesis \cite{RumbleThesis} proposed the layered architecture described in Figure \ref{fig:Rumble_General_Architecture}. It consists of 4 main phases:
\begin{enumerate}
	\item Lexer and Parser take JSONiq query as an input and produce Abstract Syntax Tree - AST as the output 
	\item Translator takes the AST as the input and produces tree of expressions - Expression Tree as the output
	\item Generator takes Expression Tree as input and converts it into tree of runtime iterators - Runtime Iterator Tree
	\item Runtime iterators represent basically the code that can be executed on single node or on top of Spark
\end{enumerate} 

\begin{figure}[h!]
	\includegraphics[width=\linewidth]{parsing_architecture.png}
	\vspace*{-5mm}
	\caption{Rumble General Architecture}
	\label{fig:Rumble_General_Architecture}
\end{figure}

\subsubsection{Lexer and Parser}
The first steps in analyzing source code, in this case query written in JSONiq query language, are Lexical and Syntax analysis's performed by Lexer and Parser modules respectively. For rather simple languages, such as JSONiq is, these two modules can be automatically generated from grammar of language. Thus, Another Tool for Language Recognition - ANTLR v4 framework \cite{ANTLR} is used. ANTLR needs grammar (.g4) file with definitions of all language constructs as the input. For Rumble, JSONiq.g4 file was implemented and using it ANTLR auto-generated Parser and Lexer together with BaseVisitor (implements visitor pattern) Java classes. In the code, you can now use first Lexer class that takes JSONiq query stream as input and then pass it to Parser class which will generate AST and conclude the so called "front-end" part of compiler.

\subsubsection{Translator}
In general with compilers, AST cannot be used directly. As explained in \cite{RumbleMLThesis}, JSONiq is functional language that is composed of expressions. Thus, higher-level abstractions - Expression Tree is needed. To achieve higher-level abstractions, following classes had to be implemented. On top of inheritance tree, we have abstract class Node from which Expression and Clause classes are derived. Clause class is then used for for deriving all clauses of FLWOR Expression. For all other Expression types mentioned in Section \ref{sec:JSONiq}, classes were derived from Expression class. 

Second part of generating Expression Tree required specific implementation of BaseVisitor class generated by ANTLR. BaseVisitor is a generic class and its specific implementation - TranslationVisitor class wraps around Node class. 

Third part of generating Expression Tree is Static Context class containing map between variable names and sequence types. Each expression has its own static context.

Using all these classes, it is then possible to generate Expression Tree as explained in \cite{RumbleThesis}: 

"The visitor starts at the top level expression and then moves through all of the children passing along the current static context while doing three things:
\begin{enumerate}
	\item For any expression that it visits, it sets the static context to be equal to the currently generated one.
	\item For any variable reference, it checks that the variable name is present in the current static context, otherwise it throws an error (at compile time).
	\item For any variable declaration it creates a new static context containing the new variable and sets the previously existing static context as parent."
\end{enumerate}


\todo{Verify that they created Node and Expression classes and use them for the generic Translation Visitor class }

\subsubsection{Generator}
\todo{Original thesis has FLWOR put in the group of Iterators that execute on top of Spark. However, ML on Rumble thesis puts FLWOR in the local runtime iterators. It is connected with previous question that was confusing me when I did the mapping}
